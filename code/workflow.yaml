# ci.yaml
tasks:
    - name: Graph metrics
      conda_env: pyspark
      cmd: spark-submit --driver-cores 18 --driver-memory 24g --num-executors 24 --master local[*] --packages graphframes:graphframes:0.6.0-spark2.3-s_2.11 production_code_in/network_analysis/graph_metrics.py 
      tags: graph_metrics_production


    - name: Plot network metrics
      conda_env: graph_env 
      input: /home/vt/extra_storage/Production/output/hist_dict.pkl
      output: /home/vt/extra_storage/Production/output_graphs/altair_hist.html
      cmd: source activate graph_env && python production_code_in/network_analysis/plot_network_stats.py
      tags: network_stats
      
      
    - name: Update MYSQL database with Pagerank user table, and friends and followers table
      conda_env: sqlenv 
      input: /home/vt/extra_storage/Production/data/production_data_in/id_name_by_pagerank.csv
      input: /home/vt/extra_storage/Production/data/production_data_in/friends_and_followers_of_central_figures/part-00000-52eab7c3-999f-4107-b476-0833ea26f087-c000.json
      output: (table) - page_rank_top5000_users
      output: (table) - page_rank_top5000_friends_and_followers_of_central_figures
      cmd: source activate sqlenv && python production_code_in/ETL/insert_pagerank_ids_to_mysql.py
      tags: ETL_insert_pagerank
      
      
    - name: Update MYSQL database with tweets
      conda_env: sqlenv 
      input: /home/vt/extra_storage/Production/output/tweets_top5000_joined_by_rank.json/part-00000-0c49311a-7471-4a52-bbd9-0ec3843b0c31-c000.json
      output: (table) - tweets
      cmd: source activate sqlenv && python production_code_in/ETL/upload_tweets_to_database.py
      tags: ETL_insert_tweets
    
    
    - name: Download tweets, check Jobs.log for job status 
      conda_env: rqenv
      input: /home/vt/extra_storage/Production/code/rq_downloader/input_userids.in
      output: /home/vt/extra_storage/twitter_data/tweets_central_figures/
      cmd: source activate rqenv && cd /home/vt/RQ/&& python /home/vt/extra_storage/Production/code/rq_downloader/job_submission.py --file /home/vt/extra_storage/Production/code/rq_downloader/input_userids.in --output /home/vt/extra_storage/twitter_data/tweets_central_figures/ --function download_tweets
      tags: download_tweets_user
      
      
    - name: Download tweets for all users in pagerank, check Jobs.log for job status 
      conda_env: rqenv
      input1: /home/vt/extra_storage/Production/data/production_data_in/id_name_by_pagerank.csv
      input2: /home/vt/extra_storage/Production/code/rq_downloader/input_userids.in
      output: /home/vt/extra_storage/twitter_data/tweets_central_figures
      cmd: awk -F"," '{print $1}' ../data/production_data_in/id_name_by_pagerank.csv > rq_downloader/input_userids.in
      cmd: cp /home/vt/extra_storage/Production/code/rq_downloader/input_userids.in /home/vt/RQ/input_userids.in
      cmd: source activate rqenv && cd /home/vt/RQ/ && python /home/vt/extra_storage/Production/code/rq_downloader/job_submission.py --file /home/vt/extra_storage/Production/code/rq_downloader/input_userids.in --output /home/vt/extra_storage/twitter_data/tweets_central_figures/ --function download_tweets
      tags: download_tweets_for_pagerank_ids
      
    #- name: Delete all files older than a certain time
      #cmd: find /path -mmin +59 -type f -exec rm -fv {} \;
     # tags: delete_files
     
     
    - name: ETL for joining tweets with the top 5000 pagerank ids and usernames
      dependency: download_tweets_for_pagerank_ids
      dependency_type: 'one_time'
      conda_env: pyspark
      input1: /home/vt/extra_storage/Production/data/production_data_in/id_name_by_pagerank.csv
      input2: /home/vt/extra_storage/twitter_data/tweets_central_figures/*.txt
      output: /home/vt/extra_storage/Production/data/temp_data_in/tweets_top5000_joined_by_rank
      cmd:  spark-submit --driver-cores 18 --driver-memory 24g --num-executors 24 --master local[*] --packages graphframes:graphframes:0.6.0-spark2.3-s_2.11 production_code_in/ETL/join_tweets_with_pagerank.py --input_pagerank_id_file /home/vt/extra_storage/Production/data/production_data_in/id_name_by_pagerank.csv
      tags: clean_tweets_join_with_pagerank

     
    - name: Topic modeling
      dependency: clean_tweets_join_with_pagerank
      dependency_type: 'one_time'
      conda_env: pyspark
      input: /home/vt/extra_storage/Production/output/tweets_top5000_joined_by_rank.json
      cmd: spark-submit --driver-cores 18 --driver-memory 24g --num-executors 24 --master local[*] --packages graphframes:graphframes:0.6.0-spark2.3-s_2.11 production_code_in/NLP/topic_modeling.py
      tags: topic_model

      
    - name: Create tweets for sentiment analyses 
      dependency: clean_tweets_join_with_pagerank 
      dependency_type: 'one_time'
      conda_env: pyspark
      input: /home/vt/extra_storage/Production/output/tweets_top5000_joined_by_rank.json
      output: /home/vt/extra_storage/Production/output/tweets_tokenized_sentiment.json
      cmd: spark-submit --driver-cores 18 --driver-memory 24g --num-executors 24 --master local[*]  production_code_in/ETL/create_tweets_for_sentiment_analyses.py 
      tags: create_tweets_for_sentiment_analyses
      comment: Move the output parquet files to Wasabi cloud bucket /sentiment/


    - name: Copy unprocessed tweet files to Wasabi
      input: /home/vt/extra_storage/Production/output/tweets_tokenized_sentiment.json/
      output: /sentiment/
      command: aws s3 cp /home/vt/extra_storage/Production/output/tweets_tokenized_sentiment.json/ s3://sentiment/ --recursive --profile wasabi --endpoint-url https://s3.wasabisys.com
      tags: copy_sentiment_files_to_wasabi


    - name: Sentiment analysis 
      dependency: create_tweets_for_sentiment_analyses 
      dependency_type: 'one_time'
      conda_env: tf_transformers/ Google Colab (transformers, sentencepiece, fastparquet, s3fs)
      input: /home/vt/extra_storage/Production/output/tweets_tokenized_sentiment.json
      input: /sentiment/*.parquet
      output: /sentimentres/results/*.npy
      cmd: python transformer_sentiment_analyzer_from_S3.py
      tags: run_tweeteval_sentiment_analyses
    


    - name: Copy processed sentiment files from Wasabi to local folder 
      input: /sentimentres/results/
      output: /home/vt/extra_storage/Production/output/processed_tweet_files/
      command: aws s3 cp s3://sentimentres/results /home/vt/extra_storage/Production/output/sentiment --recursive --profile wasabi --endpoint-url https://s3.wasabisys.com
      tags: copy_processed_sentiment_files_from_Wasabi

